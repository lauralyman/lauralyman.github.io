---
title: "Lab 7: Intro to Cross Validation"
author: "YOUR NAME HERE"
# format: live-html
# engine: knitr
format: #revealjs
    pdf:
      keep-tex: true
      include-in-header:
         text: |
           \usepackage{fvextra}
           \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
            \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
    html:
      self-contained: true
      grid:
        margin-width: 200px
      code-fold: false
      toc: true
      # callout-appearance: minimal
# You can change the color theme of the rendered document 
theme: default
---



```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE, 
  warning = FALSE,
  message = FALSE,
  fig.height = 2.75, 
  fig.width = 4.25,
  fig.env='figure',
  fig.pos = 'h',
  fig.align = 'center')
```




# R Code Tutorial notes

Goals:
* Build a linear regression model of the height (in feet) of a black cherry tree given its diameter in inches (X)
* Will use k-fold cross validation to evaluate strength of predictive model


```{r}
library(tidyverse)
library(tidymodels)
# Load data
data(trees)
# Rename columns
trees <- trees %>% rename(diameter = Girth, height = Height) %>% select(height, diameter)
```


```{r}
# Create a scatter plot
ggplot(trees, aes(x = diameter, y = height)) + geom_point() + geom_smooth(method = "lm")
```

* Height appears to be weakly (positively) correlated with diameter


```{r}
# Step 1: Model specification
lm_spec <- linear_reg() %>% 
  # Output Y is quantitative
  set_mode("regression") %>%
  # Want regression to be lienar
  set_engine("lm")

# Step 2: Model estimation
tree_model <- lm_spec %>% fit(height ~ diameter, data = trees)
```

## 10-Fold Cross Validation

* Randomly split the data into 10 folds
* Built model 10 times, leaving 1 test fold out each time
* Evaluate each model on the test fold (using MAE/MSE and R-squared)

```{r}
set.seed(244)

# fit_resamples() function is for fitting on folds
tree_model_cv = lm_spec %>% 
                fit_resamples(
                  height ~ diameter, # Specify the relationship
                  resamples = vfold_cv(trees, v = 10), # Defining the 10 folds: 10 of them and randomly selected from trees data set
                  metrics = metric_set(mae, rmse, rsq) # Define error metrics (MAE, root MSE, R^2)
                )
```


```{r}
tree_model_cv %>% collect_metrics()
```

* MAE interpretation: we expect our predictions for new trees to be off by roughly 4.3 feet (TO DO: why? what does this mean?)
* We expect for new trees that our model explains roughly 50% of the variability in height from tree to tree (TO DO: why? what does this mean?)

```{r}
# Get fold-by-fold results
# Get info for each test fold 
tree_model_cv %>% unnest(.metrics) #%>% filter(.metric == "mae")
```
* Can see (for me) prediction error (MAE) was best for fold 8 and worst for fold 3. 



```{r}

```


# Important Details

- The "Notes" sections of this document for your reference. Leave any code in 
those sections as is.
- When working on the exercises, copy-paste any starter code that I give you into a 
new code chunk, below, and edit from there.












# Context

- **Broader subject area: supervised learning**       
    We want to build a model for some output RV $Y$ given various predictor RVs $X_1, \ldots, X_p$.

- **Task: regression**       
    $Y$ is quantitative (takes numerical values)

- **Algorithm: linear regression model**       
    We'll assume that the relationship between $Y$ and $X$ can be represented by
    
    $$Y= \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \varepsilon$$
    
Taking the expected values of both sides, and using linearity of expectation, 

$$\mathbb{E}(Y \mid X_1, \ldots, X_p) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \mathbb{E}(\varepsilon) $$

where $\varepsilon \sim \mathcal{N}(0, 1)$. \textcolor{red}{TO DO: check if variance is 1 here}

\textcolor{red}{TO DO: um why not taking expected value of $X_1, \ldots, X_p$ -- should I explain that we need to be more careful with notation and actually say what $\mathbb{E}(Y \mid X)$ (conditional expectation) is??}

LL Note to Self: Search TO DO in this document
# Notes: k-fold Cross-Validation {-}


# K-Fold Cross Validation


We can use **k-fold cross-validation** to estimate the typical error in our model predictions for *new* data:

- Divide the data into $k$ folds (or groups) of approximately equal size.    
- Repeat the following procedures for each fold $j = 1,2,...,k$:    
    - Remove fold $j$ from the data set.    
    - Fit a model using the data in the other $k-1$ folds (training).    
    - Use this model to predict the responses for the $n_j$ cases in fold $j$: $\hat{y}_1, ..., \hat{y}_{n_j}$.    
    - Calculate the MAE for fold $j$ (testing): $\text{MAE}_j = \frac{1}{n_j}\sum_{i=1}^{n_j} |y_i - \hat{y}_i|$.
- Combine this information into one measure of model quality:
    $$\text{CV}_{(k)} = \frac{1}{k} \sum_{j=1}^k \text{MAE}_j$$



![](https://kegrinde.github.io/stat253_coursenotes/images/crossval.png)












# Small Group Discussion: Algorithms and Tuning {-}


**Definitions:**

- **algorithm** = a step-by-step procedure for solving a problem (Merriam-Webster)

- **tuning parameter** = a *parameter* or *quantity* upon which an algorithm depends, that must be *selected* or *tuned* to "optimize" the algorithm

![](https://c1.wallpaperflare.com/preview/461/820/840/music-low-electric-bass-strings.jpg){width=250px}^[https://www.wallpaperflare.com/grayscale-photography-of-guitar-headstock-music-low-electric-bass-wallpaper-zzbyn]
![](https://p1.pxfuel.com/preview/870/881/120/mixer-music-audio-studio-sound-studio-sound-mixer.jpg){width=250px}


**Prompts:**

1. **Algorithms**

a. Why is $k$-fold cross-validation an *algorithm*?



b. What is the *tuning parameter* of this algorithm and what values can this take?






2. **Tuning the k-fold Cross-Validation algorithm**

Let's explore k-fold cross-validation with some personal experience. Our class has a representative sample of cards from a non-traditional population (no "face cards", not equal numbers, etc). We want to use these to predict whether a new card will be odd or even (a classification task).

TO DO: Do I make this an activity or what?

a. Based on *all* of our cards, do we predict the next card will be odd or even?


b. You've been split into 2 groups. Use 2-fold cross-validation to estimate the possible error of using *our* sample of cards to predict whether a *new* card will be odd or even. How's this different than *validation*?


c. Repeat for 3-fold cross-validation. Why might this be better than 2-fold cross-validation?



d. Repeat for LOOCV, i.e. n-fold cross-validation where n is the number of students in this room. Why might this be worse than 3-fold cross-validation?



e. What value of k do you think practitioners typically use?



3. **R Code Preview**

We've been doing a 2-step process to build **linear regression models** using the **tidymodels** package:

```{r eval = FALSE}
# STEP 1: Model specification
lm_spec <- linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("lm")
  
# STEP 2: Model estimation
my_model <- lm_spec %>% 
            fit(y ~ x1 + x2, data = sample_data)
```


For k-fold cross-validation, we can *tweak* STEP 2.

Discuss the code. Why do we need to set the seed?
TO DO: change this question?

```{r eval = FALSE}
# k-fold cross-validation
set.seed(___)
my_model_cv <- lm_spec %>% 
  fit_resamples(
    y ~ x1 + x2, 
    resamples = vfold_cv(sample_data, v = ___), 
    metrics = metric_set(mae, rsq)
  )
```




# Notes: R code


Suppose we wish to build and evaluate a linear regression model of `y` vs `x1` and `x2` using our `sample_data`. 


**First, load the appropriate packages**

```{r eval = FALSE}
# Load packages
library(tidyverse)
library(tidymodels)
```




**Obtain k-fold cross-validated estimates of MAE and $R^2$**

(Review example 3 above for discussion of these steps.)


```{r eval = FALSE}
# model specification
lm_spec <- linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("lm")

# k-fold cross-validation
# For "v", put your number of folds k
set.seed(___)
model_cv <- lm_spec %>% 
  fit_resamples(
    y ~ x1 + x2,
    resamples = vfold_cv(sample_data, v = ___), 
    metrics = metric_set(mae, rsq)
)
```





**Obtain the cross-validated metrics**

```{r eval = FALSE}
model_cv %>% 
  collect_metrics()
```






**Details: get the MAE and R-squared for each test fold**

```{r eval = FALSE}
# MAE for each test fold: Model 1
model_cv %>% 
  unnest(.metrics)
```








# Exercises

```{r message = FALSE, warning = FALSE}
# Load packages and data
library(tidyverse)
library(tidymodels)
humans <- read.csv("https://kegrinde.github.io/stat253_coursenotes/data/bodyfat50.csv") %>% 
  filter(ankle < 30) %>% 
  rename(body_fat = fatSiri)
```




## EXERCISE 1: Review -- In-sample metrics

Use the `humans` data to build two separate models of `height`:

```{r eval = FALSE}
# STEP 1: model specification
lm_spec <- ___() %>% 
  set_mode(___) %>% 
  set_engine(___)
```

```{r}

```


```{r eval = FALSE}
# STEP 2: model estimation
model_1 <- ___ %>% 
  ___(height ~ hip + weight + thigh + knee + ankle, data = humans)
model_2 <- ___ %>% 
  ___(height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = humans)
```

```{r}

```





Calculate the **in-sample** R-squared for both models:

```{r eval = FALSE}
# IN-SAMPLE R^2 for model_1 = ???
model_1 %>% 
  ___()

# IN-SAMPLE R^2 for model_2 = ???
model_2 %>% 
  ___()
```

```{r}

```




Calculate the **in-sample** MAE for both models:

```{r eval = FALSE}
# IN-SAMPLE MAE for model_1 = ???
model_1 %>% 
  ___(new_data = ___) %>% 
  mae(truth = ___, estimate = ___)

# IN-SAMPLE MAE for model_2 = ???
model_2 %>% 
  ___(new_data = ___) %>% 
  mae(truth = ___, estimate = ___)
```

```{r}

```





\
\


## EXERCISE 2: In-sample model comparison

Which model seems "better" by the in-sample metrics you calculated above? Any concerns about either of these models?










\
\


## EXERCISE 3: 10-fold CV

Complete the code to run 10-fold cross-validation for our two models.

`model_1`: `height ~ hip + weight + thigh + knee + ankle`       
`model_2`: `height ~ chest * age * weight * body_fat + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist`

```{r eval = FALSE}
# 10-fold cross-validation for model_1
set.seed(253)
model_1_cv <- ___ %>% 
  ___(
    ___,
    ___ = vfold_cv(___, v = ___), 
    ___ = metric_set(mae, rsq)
  )
```

```{r}

```


```{r eval = FALSE}
# 10-fold cross-validation for model_2
set.seed(253)
model_2_cv <- ___ %>% 
  ___(
    ___,
    ___ = vfold_cv(___, v = ___), 
    ___ = metric_set(mae, rsq)
  )
```

```{r}

```
    


\
\



## EXERCISE 4: Calculating the CV MAE

a. Use `collect_metrics()` to obtain the cross-validated MAE and $R^2$ for both models.

```{r eval = FALSE}
# HINT
___ %>% 
  collect_metrics()
```
    
b. Interpret the cross-validated MAE *and* $R^2$ for `model_1`.    
    







\
\


## EXERCISE 5: Details -- fold-by-fold results

`collect_metrics()` gave the final CV MAE, or the average MAE across all 10 test folds. `unnest(.metrics)` provides the MAE from *each* test fold.

a. Obtain the fold-by-fold results for the `model_1` cross-validation procedure using `unnest(.metrics)`.       

```{r eval = FALSE}
# HINT
___ %>% 
  unnest(.metrics)
```

```{r}

```



b. Which fold had the worst average prediction error and what was it?



c. Recall that `collect_metrics()` reported a final CV MAE of 1.87 for `model_1`. Confirm this calculation by wrangling the fold-by-fold results from part a.



   
    
    
    


\
\





## EXERCISE 6: Comparing models

The table below summarizes the in-sample and 10-fold CV MAE for both models.    
    
Model        IN-SAMPLE MAE  10-fold CV MAE
----------- -------------- ---------------
`model_1`             1.55            1.87
`model_2`             0.64            2.47


a. Based on the in-sample MAE alone, which model appears better?    


b. Based on the CV MAE alone, which model appears better?    


c. Based on all of these results, which model would you pick?


d. Do the in-sample and CV MAE suggest that `model_1` is overfit to our `humans` sample data? What about `model_2`?





\
\


## EXERCISE 7: LOOCV

a. Reconsider `model_1`. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our `humans` sample?



b. How does the LOOCV MAE compare to the 10-fold CV MAE of 1.87? NOTE: These are just two different *approaches* to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.    



c. Explain why we technically don't *need* to `set.seed()` for the LOOCV algorithm.





\
\



## EXERCISE 8: Data drill

a. Calculate the average height of people under 40 years old vs people 40+ years old.

```{r}

```

b. Plot height vs age among our subjects that are 30+ years old.

```{r}

```

c. Fix this code:       

```{r eval = FALSE}
model_3<-lm_spec%>%fit(height~age,data=humans)
model_3%>%tidy()
```

```{r}

```





## EXERCISE 9: Reflection 


- What are the four main questions that were important to this unit?

- For each of the following tools, describe how they work and what questions they help us address:        
    - R-squared
    - residual plots
    - MAE
    - in-sample MAE
    - validation
    - cross-validation

- In your own words, define the following: overfitting, algorithm, tuning parameter.

- Review the new tidymodels syntax from this unit. Identify key themes and patterns.




    






    