---
title: "Lab 6: Intro to Model Evaluation"
author: "YOUR NAME HERE"
# format: live-html
# engine: knitr
format: #revealjs
    pdf:
      keep-tex: true
      include-in-header:
         text: |
           \usepackage{fvextra}
           \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
            \DefineVerbatimEnvironment{OutputCode}{Verbatim}{breaklines,commandchars=\\\{\}}
    html:
      self-contained: true
      grid:
        margin-width: 200px
      code-fold: false
      toc: true
      # callout-appearance: minimal
# You can change the color theme of the rendered document 
theme: default
---

```{r include = FALSE}
knitr::opts_chunk$set(
  collapse = FALSE, 
  warning = FALSE,
  message = FALSE,
  fig.height = 2.75, 
  fig.width = 4.25,
  fig.env='figure',
  fig.pos = 'h',
  fig.align = 'center',
  echo  = TRUE,
  error = TRUE)
```

::: {.callout-tip collapse="true" icon="true"}
# Getting Started

1.  Download the `.qmd` file from Moodle and any needed `.xlsx` or `.csv` data files. Save these in the *same folder/directory*. 

2.  Open the Quarto file in RStudio: `File > Open File... >`. (If you're working on the MHC RStudio server, you need to upload the file first: go to the `Files` panel, then click `Upload`. Upload both the `.qmd` file and any data files.)

3.  Update the author and date in the YAML header of this file.

4.  Click the `Render` button. If successful, you should have a new window pop up with a nice looking HTML document.

**Ask for help** if you encounter issues on any of the steps above. Once you've successfully made it through these steps, you can continue.
:::

## Learning Goals

-   Implement testing and training sets in R using the `tidymodels` package

-   See why training (*in-sample*) model evaluation metrics can provide a *misleading* view of true test (*out-of-sample*) performance

## Overview

-   Let's *build* and *evaluate* a predictive model of an adult's height ($Y$) using some predictors $X_i$ (eg: age, height, etc)

-   Since $Y$ is *quantitative*, this is a regular **regression task**

-   There are countless possible models of $Y$ vs $X$. We'll utilize a **linear regression model**:

$$\mathbb{E}(Y \mid X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$$

-   And after building this model, we'll **evaluate** it

## Load the Data

```{r}
# Load packages needed for this analysis
library(readxl)
library(tidyverse)
library(tidymodels)
library(openxlsx)
```

::: {.callout-tip collapse="true"}
# Warm Up

The `nchar()` function returns the number of characters in a string. For example, `nchar("Laura")` returns 5. Use `nchar` to compute the number of letters in your *first name* modulo 5.\
\
As a reminder, `%%` in R is equivalent to "mod".

```{r}
# YOUR CODE HERE
nchar("Laura") %% 5
```
:::

Each person will potentially be given a **different** sample of 40 adults.

::: {.callout-tip collapse="true"}
# Exercise: Load Data

**Fill in the blank** (\_\_\_) **at the end of the URL** with the calculated number of letters in your first name modulo 5.

```{r echo = T, results = 'hide'}
# Load your data: fill in the blank at end of the URL with your number
url_start = "https://github.com/lauralyman/lauralyman.github.io/blob"
url_end   = "/main/data/healthdata0.xlsx?raw=true"
(data_url = paste0(url_start,url_end))
healthdata = read.xlsx(data_url)
```

```{r}
# Check out a density plot of your outcomes
ggplot(healthdata, aes(x = height)) + geom_density()
```
:::

## Build the Model

Build a linear regression model of `height` (in) by `hip` circumference (cm).

```{r}
# STEP 1: Model specification
lm_spec <- linear_reg() %>% set_mode('regression') %>% set_engine('lm')
```

**Fill in the blanks in the code below.**

```{r}
# STEP 2: Model estimation
model_1 <- lm_spec %>% fit(height ~ hip, data = healthdata)
```

```{r}
# Look at the estimated beta coefficients
# Does everyone have the same coefficients? Should they?
tidy(model_1)
```

## Model Evaluation

How "good" is our model?

```{r}
# Calculate the R^2 for model_1
glance(model_1)
```

```{r eval = FALSE}
# Use your model to predict height for YOUR sample
# Just print the first 6 results
model_1 %>% augment(new_data = healthdata) %>% 
  head()
```

-   The `augment()` function takes a given model and outputs the values that model predicts for a given data set.

-   The *MAE* (*mean absolute error*) is a common metric for measuring prediction error. We can use the function `mae()` in R to compute the MAE for our model predictions.

    -   The argument `truth` is the name of the column of our actual observed outputs ($y_i$)
    -   The argument `estimate` is the name of the column with our predicted values ($\hat{y}_i$)

```{r eval = FALSE}
# Calculate MAE for YOUR model
model_1 %>% 
  augment(new_data = healthdata) %>% 
  mae(truth = height, estimate = .pred)
```

::: {.callout-tip collapse="true"}
# Exercise

In addition to `hip` circumference, suppose we incorporated more predictors into our model of `height`. What is your **guess** for what will happen to $R^2$? To the MAE? It's okay if your guess doesn't turn out to be correct!

> As we add predictors, $R^2$ is always guaranteed to not increase (not get worse). Similarly, the MAE is going to go down. 
:::

## Compare Different Models

::: {.callout-tip collapse="true"}
# Exercise

Consider 3 different models for estimating `height`. Use *your* data to choose which is the best predictive model of `height`. Calculate the MAE for each model.

```{r}
# height vs hip
model_1 <- lm_spec %>% fit(height ~ hip, data = healthdata)

tidy(model_1)
```

```{r}
# height vs hip & weight
model_2 <- lm_spec %>% fit(height ~ hip + weight, data = healthdata)

tidy(model_2)
```

```{r}
# height vs a lot of predictors (AND some interaction terms)
model_3 <- lm_spec %>% 
  fit(height ~ chest * age * weight  * abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, 
      data = healthdata)

tidy(model_3)
```

```{r}
# Calculate the MAE for each model based on YOUR data
model_2 %>% 
  augment(new_data = healthdata) %>% 
  mae(truth = height, estimate = .pred)

model_3 %>% 
  augment(new_data = healthdata) %>% 
  mae(truth = height, estimate = .pred)
      

```

When you have calculated the MAE, raise your hand, and let Laura know what you got!
:::

**PAUSE HERE**

**SERIOUSLY**

**REALLY**

**TAKE A MOMENT**

```{verbatim, include = FALSE}

            / _)
     _.----._/ /
    /         /
 __/ (  | (  |
/__.-'|_|--|_|



























                  / _)
         _.----._/ /
        /         /
     __/ (  | (  |
    /__.-'|_|--|_|       
                                    
                                                         
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
           
              
              
                         boing         boing         boing              
               e-e           . - .         . - .         . - .          
              (\_/)\       '       `.   ,'       `.   ,'       .        
               `-'\ `--.___,         . .           . .          .       
                  '\( ,_.-'                                             
                     \\               "             "            a:
                     
       
       
       
       
       
       
       






















                        _.-'           ~-_       _.=a~~-_
--=====-.-.-_----------~   .--.       _   -.__.-~ ( ___===>
              '''--...__  (    \ \\\ { )       _.-~
                        =_ ~_  \\-~~~//~~~~-=-~
                         |-=-~_ \\   \\
                         |_/   =. )   ~}
                         |}      ||
                        //       ||
                      _//        {{
                   '='~'          \\_    
                                   ~~'
                                   
                                   
       
```

**OKAY GREAT**

What do you know?! 40 new people just walked into the doctor's office, and the doctor wants to predict their heights:

```{r}
# Import the new data

url_start = "https://github.com/lauralyman/lauralyman.github.io/blob"
url_end   = "/main/data/healthdata182.xlsx?raw=true"
data_url  = paste0(url_start,url_end)
new_patients = read.xlsx(data_url)
```

::: {.callout-tip collapse="true"}
# Exercise: Intuition

Consider using *your* model to predict `height` for these 40 *new* subjects. On average, do you think these predictions will be better or worse than for your original patients? Why?

> Our guess is that for the new data points, our predictions will be worse. 

:::

Now use *your* model to predict `height` for the *new* patients and calculate the typical prediction error (MAE).

```{r}
# YOUR CODE HERE
model_3 %>% 
  augment(new_data = new_patients) %>% 
  mae(truth = height, estimate = .pred)
```


For my data set, the MAE for model 1 (only 1 predictor) is about 1.71

For my data set, the MAE for model 2 (2 predictors) is about 1.67.

For my data set, the MAE for model 3 (a BUNCH of predictors) is about 1.668894.



::: {.callout-tip collapse="true"}
# Exercise

In summary, which model seems best? What's the central theme here?

> We want to test how "effective" our model is at predicting on data that _differs_ from what was used to build the model in the first place. 

:::

## Overfitting

When we add more and more predictors into a model, it can become *overfit* to the noise in our sample data:

-   our model loses the broader trend / big picture
-   thus does not generalize to *new* data
-   thus results in bad predictions and a bad understanding of the relationship among the new data points

### Preventing Overfitting: Training and Testing

<div>

> *In-sample metrics* are measures of how well the model performs on the same sample data that was used to build it.

</div>

-   In-sample metrics tend to be overly optimistic and lead to overfitting.

-   Instead, we should *build* and *evaluate*, or **train** and **test**, our model using different data.

The following exercises are inspired by Chapter 5.3.1 of ISLR.

```{r}
# NOTE: You might first need to install the ISLR package
library(ISLR)
data(Auto)

cars <- Auto %>% select(mpg, horsepower, year)
```

Let's use the `cars` data to compare three **linear regression models** of fuel efficiency in miles per gallon (`mpg`) by engine power (`horsepower`):

```{r}
# Raw data
cars_plot <- ggplot(cars, aes(x = horsepower, y = mpg)) + geom_point()

cars_plot
```

```{r}
# Model_1: 1 predictor (Y = b0 + b1 X)
cars_plot + geom_smooth(method = "lm", se = FALSE)
```

```{r}
# Model_2: 2 predictors (Y = b0 + b1 X + b2 X^2)
cars_plot + geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 2))
```

```{r}
# Model_3: 19 predictors (Y = b0 + b1 X + b2 X^2 + ... + b19 X^19)
cars_plot + geom_smooth(method = "lm", se = FALSE, formula = y ~ poly(x, 19))
```

### Procedure

We will evaluate and compare these models by **training** and **testing** them using different data.

To do so, let's *uniformly at random* split our original 392 sample cars into two separate pieces: select 80% of the cars to **train (build)** the model and the other 20% to **test (evaluate)** the model.

```{r}
# Set the random number seed
set.seed(8)

# Split the cars data into 80% / 20%
# Ensure that the sub-samples are similar with respect to mpg
cars_split = initial_split(cars, strata = mpg, prop = .8)
```

```{r}
# Get the training data from the split
cars_train = training(cars_split)

# Get the testing data from the split
cars_test = testing(cars_split)
```

```{r}
# The original data has 392 cars
nrow(cars)
# How many cars are in the training set?
nrow(cars_train)
# How many cars are in the testing set?
nrow(cars_test)
```

::: {.callout-tip collapse="true"}
# Exercise

(1) Why do we want the training and testing data to be similar with respect to `mpg` (`strata = mpg`)? What if they *weren't*?

> This could be bad. Imagine that the testing data had the top 20\% highest values of `mpg` and the training data is the other 80\%. This could be misleading, because there are structural differences between `mpg` in the two groups. 

(2) Why did we need all this new code instead of just using the *first* 80% of cars in the sample for training and the *last* 20% for testing?

> Because we don't know if the rows in the data set are randomly ordered observations. What if the data set is ordered from lowest to highest `mpg` by row (bigger row => bigger `mpg`)? 


:::

### Build the Training Model

```{r eval = FALSE}
# Already defined lm_spec before! 

# STEP 2: Model estimation using the TRAINING data
# Construct the 19th order polynomial model using the TRAINING data
model_19_train <- lm_spec %>% 
  fit(mpg ~ poly(horsepower, 19), data = cars_train)
```

### Evaluate the Training Model

```{r eval = FALSE}
# How well does the TRAINING model predict the TRAINING data?
# Calculate the training (in-sample) MAE
model_19_train %>% 
  augment(new_data = cars_train) %>% 
  mae(truth = mpg, estimate = .pred)

# How well does the TRAINING model predict the TEST data?
# Calculate the test MAE

model_19_train %>% 
  augment(new_data = cars_test) %>% 
  mae(truth = mpg, estimate = .pred)


```

### Summary

The table below summarizes your results for `train_model_19` as well as the other two models of interest. (You should confirm the other two model results outside of class!)

| Model                        | Training MAE | Testing MAE |
|:-----------------------------|-------------:|------------:|
| `mpg ~ horsepower`           |         3.78 |        4.00 |
| `mpg ~ poly(horsepower, 2)`  |         3.20 |        3.49 |
| `mpg ~ poly(horsepower, 19)` |         2.99 |        6.59 |

::: {.callout-tip collapse="true"}
# Exercise

Answer the following and reflect on why each answer *makes sense* to you.

(1) *Within* each model, how do the training errors compare to the testing errors? (This isn't *always* the case, but it is common.)

> Training error is lower than testing error in this case! 

(2) What about the training and test errors for the third model suggest that it is *overfit* to our sample data?

> The third model has its MAE jump DRAMATICALLY between the training data set and the testing data set. It also goes from "the best" to "the worst" depending on which data set (training or testing) we are using to evaluate its performance.

(3) Which model seems the best with respect to the *training* errors?

> It looks like the third model (polynomial chaos) is the best

(4) Which model is the best with respect to the *testing* errors?

> It looks like the second model (two predictors) is the best

(5) Which model would you choose?

> YOUR ANSWER HERE
:::
