[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website!",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "My name is Professor Laura Lyman, and I am a professor at Mount Holyoke College!\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "Intro-CV.html",
    "href": "Intro-CV.html",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue."
  },
  {
    "objectID": "Intro-CV.html#context",
    "href": "Intro-CV.html#context",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Context",
    "text": "Context\n\nBroader subject area: supervised learning\nWe want to build a model for some output RV \\(Y\\) given various predictor RVs \\(X_1, \\ldots, X_p\\).\nTask: regression\n\\(Y\\) is quantitative (takes numerical values)\nAlgorithm: linear regression model\nWe’ll assume that the relationship between \\(Y\\) and \\(X\\) can be represented by\n\n\\[\\mathbb{E}(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p  \\]"
  },
  {
    "objectID": "Intro-CV.html#review-k-fold-cross-validation",
    "href": "Intro-CV.html#review-k-fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Review: \\(k\\)-Fold Cross Validation",
    "text": "Review: \\(k\\)-Fold Cross Validation\nWe can use k-fold cross-validation to estimate the typical error in our model predictions for new data:\n\nDivide the data into \\(k\\) folds (or groups) of approximately equal size.\n\nRepeat the following procedures for each fold \\(j = 1,2,...,k\\):\n\nRemove fold \\(j\\) from the data set.\n\nFit a model using the data in the other \\(k-1\\) folds (training).\n\nUse this model to predict the responses for the \\(n_j\\) cases in fold \\(j\\): \\(\\hat{y}_1, ..., \\hat{y}_{n_j}\\).\n\nCalculate the MAE for fold \\(j\\) (testing): \\(\\text{MAE}_j = \\frac{1}{n_j}\\sum_{i=1}^{n_j} |y_i - \\hat{y}_i|\\).\n\nCombine this information into one measure of model quality: \\[\\text{CV}_{(k)} = \\frac{1}{k} \\sum_{j=1}^k \\text{MAE}_j\\]\n\n\n\n\n\n\n\n\nVocabulary\n\n\n\n\n\n\nA tuning parameter is parameter or quantity upon which an algorithm depends whose value is selected or tuned to “optimize” the algorithm.\n\nFor \\(k\\)-fold CV, the tuning parameter is \\(k\\), where \\(2 \\le k \\le n\\), where \\(n\\) is the number of observations.\nHeuristic: \\(k\\) is usually picked to be something in the middle."
  },
  {
    "objectID": "Intro-CV.html#set-up",
    "href": "Intro-CV.html#set-up",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "Set Up",
    "text": "Set Up\n\nLoad Libraries\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(readxl)\n# Load data\ndata(trees)\n\n\n\nRename Columns (Variables)\n\nRename Girth to diameter\nRename Height to height\n\n\n# Rename columns\ntrees = trees %&gt;% \n  rename(diameter = Girth, height = Height) %&gt;%\n  # Only have height and diameter be the columns \n  select(height, diameter)\n\n\n\nVisualization (Scatter Plot)\n\n# Create a scatter plot\nggplot(trees, aes(x = diameter, y = height)) + \n  geom_point() + \n  geom_smooth(method = \"lm\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nHeight appears to be (weakly) positively correlated with diameter\n\n\n\n\n\n# Step 1: Model specification\nlm_spec &lt;- linear_reg() %&gt;% \n  # Output Y is quantitative\n  set_mode(\"regression\") %&gt;%\n  # Want regression to be lienar\n  set_engine(\"lm\")\n\n\n# Step 2: Model estimation\ntree_model &lt;- lm_spec %&gt;% \n  fit(height ~ diameter, data = trees)"
  },
  {
    "objectID": "Intro-CV.html#fold-cross-validation",
    "href": "Intro-CV.html#fold-cross-validation",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "10-Fold Cross Validation",
    "text": "10-Fold Cross Validation\n\nProcedure\n\nRandomly split the data into 10 folds\nBuilt model 10 times, leaving 1 test fold out each time\nEvaluate each model on the test fold (using MAE/MSE and R-squared as error metrics)\n\n\n# For reproducibility\nset.seed(242)\n\ntree_model_cv = lm_spec %&gt;% \n# fit_resamples() function is for fitting on folds\nfit_resamples(\n  # Specify the relationship\n  height ~ diameter, \n  # vfold_cv makes CV folds randomly from\n  # trees data set\n  resamples = vfold_cv(trees, v = 10), \n  # Specify the error metrics\n  # (MAE, square root MSE, R^2)\n  metrics = metric_set(mae, rmse, rsq)\n  )\n\n\ntree_model_cv %&gt;% collect_metrics()\n\n# A tibble: 3 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 mae     standard   4.38     10   0.639 Preprocessor1_Model1\n2 rmse    standard   5.17     10   0.733 Preprocessor1_Model1\n3 rsq     standard   0.341    10   0.121 Preprocessor1_Model1\n\n\n\n\nSummarizing\n\n# Get fold-by-fold results\n# Get info for each test fold \ntree_model_cv %&gt;% unnest(.metrics) %&gt;% \nfilter(.metric == \"mae\")\n\n# A tibble: 10 × 7\n   splits         id     .metric .estimator .estimate .config           .notes  \n   &lt;list&gt;         &lt;chr&gt;  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;             &lt;list&gt;  \n 1 &lt;split [27/4]&gt; Fold01 mae     standard        4.23 Preprocessor1_Mo… &lt;tibble&gt;\n 2 &lt;split [28/3]&gt; Fold02 mae     standard        5.67 Preprocessor1_Mo… &lt;tibble&gt;\n 3 &lt;split [28/3]&gt; Fold03 mae     standard        7.92 Preprocessor1_Mo… &lt;tibble&gt;\n 4 &lt;split [28/3]&gt; Fold04 mae     standard        3.59 Preprocessor1_Mo… &lt;tibble&gt;\n 5 &lt;split [28/3]&gt; Fold05 mae     standard        6.11 Preprocessor1_Mo… &lt;tibble&gt;\n 6 &lt;split [28/3]&gt; Fold06 mae     standard        1.42 Preprocessor1_Mo… &lt;tibble&gt;\n 7 &lt;split [28/3]&gt; Fold07 mae     standard        1.99 Preprocessor1_Mo… &lt;tibble&gt;\n 8 &lt;split [28/3]&gt; Fold08 mae     standard        2.76 Preprocessor1_Mo… &lt;tibble&gt;\n 9 &lt;split [28/3]&gt; Fold09 mae     standard        5.70 Preprocessor1_Mo… &lt;tibble&gt;\n10 &lt;split [28/3]&gt; Fold10 mae     standard        4.37 Preprocessor1_Mo… &lt;tibble&gt;\n\n\n\n\n\n\n\n\nObservations\n\n\n\n\n\n\nBased on my random folds above, the prediction error (MAE) was best for fold 7 and worst for fold 3."
  },
  {
    "objectID": "Intro-CV.html#exercise-1-in-sample-metrics",
    "href": "Intro-CV.html#exercise-1-in-sample-metrics",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 1: In-Sample Metrics",
    "text": "EXERCISE 1: In-Sample Metrics\nUse the health_data data to build two separate models of height:\n\n# STEP 2: model estimation\nmodel_1 &lt;- lm_spec %&gt;% \n  fit(height ~ hip + weight + thigh + knee + ankle, data = health_data)\n\nError in is_list(data): object 'health_data' not found\n\nmodel_2 &lt;- lm_spec %&gt;% \n  fit(height ~ chest * age * weight + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist, data = health_data)\n\nError in is_list(data): object 'health_data' not found\n\n\nCalculate the in-sample R-squared for both models:\n\n# IN-SAMPLE R^2 for model_1 = ???\nmodel_1 %&gt;% glance()\n\nError in glance(.): object 'model_1' not found\n\n# IN-SAMPLE R^2 for model_2 = ???\nmodel_2 %&gt;% glance()\n\nError in glance(.): object 'model_2' not found\n\n\n\nANSWER. The R2 value for the first model is about 0.366, and for the second model, it’s 0.526.\n\nCalculate the in-sample MAE for both models:\n\n# IN-SAMPLE MAE for model_1 = ???\nmodel_1 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\nError in augment(., new_data = health_data): object 'model_1' not found\n\n# IN-SAMPLE MAE for model_2 = ???\nmodel_2 %&gt;% \n  augment(new_data = health_data) %&gt;% \n  mae(truth = height, estimate = .pred)\n\nError in augment(., new_data = health_data): object 'model_2' not found\n\n\n\nANSWER. Based on the in-sample MAE (i.e., the MAE of the same data used to build/train the model), it appears that model 2 (whose MAE is about 3.366) is better than model 1 (whose MAE is about 3.481)."
  },
  {
    "objectID": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "href": "Intro-CV.html#exercise-2-in-sample-model-comparison",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 2: In-Sample Model Comparison",
    "text": "EXERCISE 2: In-Sample Model Comparison\nWhich model seems “better” by the in-sample metrics you calculated above? Any concerns about either of these models?\n\nAnswered above! The concern is that we’re using the same data that we built the model with to evaluate the model’s error/performance, which means that the “better looking” model might be overfit (might be overly-specific to the data used to build it)."
  },
  {
    "objectID": "Intro-CV.html#exercise-3-10-fold-cv",
    "href": "Intro-CV.html#exercise-3-10-fold-cv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 3: 10-Fold CV",
    "text": "EXERCISE 3: 10-Fold CV\nComplete the code to run 10-fold cross-validation for our two models.\nmodel_1: height ~ hip + weight + thigh + knee + ankle\nmodel_2: height ~ chest * age * weight  + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist\n\n# 10-fold cross-validation for model_1\nset.seed(244)\nmodel_1_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\nError in nrow(data): object 'health_data' not found\n\n\n\n# 10-fold cross-validation for model_2\nset.seed(253)\nmodel_2_cv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ chest * age * weight + abdomen + hip + thigh + knee + ankle + biceps + forearm + wrist,\n    resamples = vfold_cv(health_data, v = 10), \n    metrics = metric_set(mae, rsq)\n  )\n\nError in nrow(data): object 'health_data' not found"
  },
  {
    "objectID": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "href": "Intro-CV.html#exercise-4-calculating-the-cv-mae",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 4: Calculating the CV MAE",
    "text": "EXERCISE 4: Calculating the CV MAE\n\nUse collect_metrics() to obtain the cross-validated MAE and \\(R^2\\) for both models.\n\n\nmodel_1_cv %&gt;% \n  collect_metrics()\n\nError in collect_metrics(.): object 'model_1_cv' not found\n\nmodel_2_cv %&gt;% \n  collect_metrics()\n\nError in collect_metrics(.): object 'model_2_cv' not found\n\n\n\nInterpret the cross-validated MAE and \\(R^2\\) for model_1.\n\n\nANSWER. We expect our first model to produce predictions of height that are roughly off by 4.13 (the observed MAE) on average. For the first model, we expect it to explain roughly 0.28 (28%) of the variability (based on the R2 value) in the observed heights of patients in the data set."
  },
  {
    "objectID": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "href": "Intro-CV.html#exercise-5-fold-by-fold-results",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 5: Fold-By-Fold Results",
    "text": "EXERCISE 5: Fold-By-Fold Results\nThe command collect_metrics() gave the final CV MAE, or the average MAE across all 10 test folds. The command unnest(.metrics) provides the MAE from each test fold.\n\nObtain the fold-by-fold results for the model_1 cross-validation procedure using unnest(.metrics).\n\n\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;% \n  filter(.metric == \"mae\")\n\nError in unnest(., .metrics): object 'model_1_cv' not found\n\n\n\nWhich fold had the worst average prediction error and what was it?\n\n\nFor me, fold 5 had the worst (highest) MAE (which was 10.9).\n\n\nRecall that collect_metrics() reported a final CV MAE of 4.13 for model_1. Confirm this calculation by wrangling the fold-by-fold results from part a.\n\n\n# Code here\nmodel_1_cv %&gt;% \n  unnest(.metrics) %&gt;% \n  filter(.metric == \"mae\") %&gt;% \n  summarize(mean(.estimate))\n\nError in unnest(., .metrics): object 'model_1_cv' not found"
  },
  {
    "objectID": "Intro-CV.html#exercise-6-comparing-models",
    "href": "Intro-CV.html#exercise-6-comparing-models",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 6: Comparing Models",
    "text": "EXERCISE 6: Comparing Models\nFill in the table below to summarize the in-sample and 10-fold CV MAE for both models.\n\n\n\nModel\nIN-SAMPLE MAE\n10-fold CV MAE\n\n\n\n\nmodel_1\n3.48\n4.13\n\n\nmodel_2\n3.37\n6.28\n\n\n\n\nBased on the in-sample MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on the CV MAE alone, which model appears better?\n\n\nYOUR ANSWER HERE\n\n\nBased on all of these results, which model would you pick?\n\n\nYOUR ANSWER HERE\n\n\nDo the in-sample and CV MAE suggest that model_1 is overfit to our health_data sample data? What about model_2?\n\n\nFor model 1, it looks like the MAE is roughly similar for when it’s measured in-sample (3.48) versus when it’s tested on “new” data (each test fold held out) (4.13). However, model 2 seems overfit because its predictions for new patient data (giving an MAE of 6.28) are much worse than its predictions for patients in our data sample (MAE of 3.37)."
  },
  {
    "objectID": "Intro-CV.html#exercise-7-loocv",
    "href": "Intro-CV.html#exercise-7-loocv",
    "title": "Lab 7: Implementing Cross Validation (CV)",
    "section": "EXERCISE 7: LOOCV",
    "text": "EXERCISE 7: LOOCV\n\nReconsider model_1. Instead of estimating its prediction accuracy using the 10-fold CV MAE, use the LOOCV MAE. THINK: How many people are in our health_data sample?\n\n\n# CODE HERE\n\nmodel_1_loocv &lt;- lm_spec %&gt;% \n  fit_resamples(\n    height ~ hip + weight + thigh + knee + ankle,\n    resamples = vfold_cv(health_data, v = nrow(health_data)), \n    metrics = metric_set(mae)\n  )\n\nError in nrow(data): object 'health_data' not found\n\n\n\nmodel_1_loocv %&gt;% collect_metrics()\n\nError in collect_metrics(.): object 'model_1_loocv' not found\n\n\n\nHow does the LOOCV MAE compare to the 10-fold CV MAE of ___? NOTE: These are just two different approaches to estimating the same thing: the typical prediction error when applying our model to new data. Thus we should expect them to be similar.\n\n\nANSWER.\n\n\nExplain why we technically don’t need to set.seed() for the LOOCV algorithm.\n\n\nANSWER."
  }
]