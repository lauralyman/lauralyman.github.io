[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my homepage!",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is my website! I am a professor at Mount Holyoke College in the Math & Stats Department.\nThis website is for STAT 244.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "K-Means-Mini-Demo.html",
    "href": "K-Means-Mini-Demo.html",
    "title": "Lab: K-Means",
    "section": "",
    "text": "Getting Started\n\n\n\n\n\n\nDownload the .qmd file from Moodle and any needed .xlsx or .csv data files. Save these in the same folder/directory.\nOpen the Quarto file in RStudio: File &gt; Open File... &gt;. If you’re working on the MHC RStudio server, you need to upload the files first: go to the Files panel, then click Upload. Upload the .qmd file and any data files. You will need to upload each file one at a time.\nUpdate the author and date in the YAML header of this file.\nClick the Render button. If successful, you should have a new window pop up with a nice looking HTML document.\nFor this lab, you may need to still the package glmnet.\n\nAsk for help if you encounter issues on any of the steps above. Once you’ve successfully made it through these steps, you can continue.\n\n\n\n\nLoad Packages\nYou likely will need to install some these packages before you can run the code chunk below successfully.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\nlibrary(factoextra)\nlibrary(amap)\n\n\n\nLoad Penguin Data\n\ndata(penguins)\n\n\n\nData Cleaning\n\n# Remove missing values\n# YOUR CODE HERE\npenguins &lt;- penguins %&gt;% \n            filter(!is.na(bill_length_mm) & !is.na(bill_depth_mm) & !is.na(species))\n# Make data table (named penguins_reduced) that only has\n# bill_length_mm and bill_depth_mm columns\npenguins_reduced &lt;- penguins %&gt;% select(bill_length_mm, bill_depth_mm)\n\n\n\nInitial Visualization\n\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) + \n  geom_point() \n\n\n\n\n\n\n\n\nWe’ll cluster these penguins based on their bill lengths and depths:\n\n\nImplement \\(K\\)-Means\nComplete the code below to run the K-means algorithm using K = 3.\n\nset.seed(244)\n# Run the K-means algorithm\nkmeans_3_round_1 &lt;- kmeans(scale(penguins_reduced), centers = 3) \n    \n# Plot the cluster assignments\npenguins_reduced %&gt;% \n  mutate(kmeans_cluster = as.factor(kmeans_3_round_1$cluster)) %&gt;%\n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = kmeans_cluster)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"K-means with K = 3 (round 1)\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWhy do we have to set the seed for K-means? In practice, why should we try out a variety of seeds?\n\nAnswer. For reproducibility. Every time we run the function kmeans, it initializes the centers of the clusters to be in random locations. It’s possible that some random locations give better clustering results than others; since k-means is a greedy algorithm, it’s possible for it to have different results each time, and it’s possible for it to get “stuck” at a local solution.\n\n\nK-Means Clusters Versus Known Species Groupings\n\n  ggplot(data = penguins, aes(x = bill_length_mm, y = bill_depth_mm, color = species)) + \n  geom_point(size = 3) + \n  theme(legend.position = \"none\") + \n  labs(title = \"Actual Groupings of Data Based on Species\") + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\nVisually, how well do you think \\(K\\)-means captured the underlying species structure of the data?\n\nAnswer. I think it did pretty good! It found roughly the data points corresponding to each of the species groups, so it is identifying that as a way to structure the data.\n\n\nTuning \\(K\\)\n\nTo implement K-means clustering we must choose an appropriate K! Use the following example to see the two different extreme situations. Typically, the ideal \\(K\\) is somewhere between the two extremes.\nMinimum: \\(K = 2\\) groups/clusters\nMaximum: \\(K = n\\) groups/clusters (one observation per cluster)\n\nWhat happens in the \\(K\\)-means algorithm if \\(K = n\\)?\nAnswer. YOUR ANSWER HERE\nLet’s consider anywhere from \\(K = 2\\) to \\(K = 20\\) clusters.\n\nset.seed(244)\n\nk_2  &lt;- kmeans(scale(penguins_reduced), centers = 2)\nk_20 &lt;- kmeans(scale(penguins_reduced), centers = 20)\n\npenguins_reduced %&gt;% \n  mutate(cluster_2 = as.factor(k_2$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_2)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 2\")\n\n\n\n\n\n\n\npenguins_reduced %&gt;% \n  mutate(cluster_20 = as.factor(k_20$cluster)) %&gt;% \n  ggplot(aes(x = bill_length_mm, y = bill_depth_mm, color = cluster_20)) + \n    geom_point(size = 3) + \n    labs(title = \"K = 20\") + \n    scale_color_manual(values = rainbow(20))\n\n\n\n\n\n\n\n\nWhat are your general impressions?\nAnswer. YOUR ANSWER HERE\n\n\nFinding Ideal K Value: Silhoutte\n\nThe average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster.\n\nTo do so, it maximizes the distance between clusters and minimizes distance within clusters.\n\nA high average silhouette indicates a good clustering.\nGiven a range of possible K values, the optimal number of clusters (K) is the one that maximizes the average silhouette.\n\nWe can use a built-in silhouette method in the fviz_nbclust function to compute the average silhouette for various K values.\n\nfviz_nbclust(scale(penguins_reduced), kmeans, method='silhouette')\n\n\n\n\n\n\n\n\nBased on the average silhouette approach, what is the optimal \\(K\\) value?\nAnswer. The optimal value for \\(K\\) appears to be \\(K\\) = 3.\n\n\nExperimenting with Distance Metrics\nWe can use the Kmeans method (notice the “K” is capitalized in this function name) from the amap library to specify how we are measuring distance in the K-means algorithm.\n\nset.seed(244)\nk_2_manattan = Kmeans(scale(penguins_reduced), centers = 3, \n                      method = \"manhattan\")\nk_2_euclid = Kmeans(scale(penguins_reduced), centers = 3, \n                    method = \"euclidean\")\nk_2_maxnorm = Kmeans(scale(penguins_reduced), centers = 3, \n                     method = \"maximum\")\n\n\n\nfviz_cluster(k_2_euclid, data = scale(penguins_reduced), \n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_manattan, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\nfviz_cluster(k_2_maxnorm, data = scale(penguins_reduced),\n             main = sprintf(\"K = %d Clusters w/ Manhattan Distance\", 3))\n\n\n\n\n\n\n\n\nTry changing \\(K\\) to equal 3$ in the code chunk above. How do the clusterings using the 3 distance metrics compare? What do you generally observe?\nAnswer. YOUR ANSWER HERE\nModify the code in the chunk above so that we can easily change the value of K (rather than making sure to change K manually in every line). In general coding practices, is called extracting out a constant."
  }
]